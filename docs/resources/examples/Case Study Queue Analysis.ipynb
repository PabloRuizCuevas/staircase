{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Case study: queue analysis\r\n",
    "\r\n",
    "This case study illustrates the use of the staircase package for queue analysis.  In this example we have a number of vessels (i.e. ships) which arrive offshore and await their turn to enter a harbour where they will be loaded with cargo.  We will examine the queue, which is composed of all vessels which are offshore but yet to enter the harbour, for the year 2020.\r\n",
    "\r\n",
    "The data used is this case study is synthetic and fictional.  Both data and the notebook for this tutorial can be obtained from the [github site](https://github.com/venaturum/staircase/tree/master/docs/examples)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import staircase as sc\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin by importing the queue data into a pandas.DataFrame instance. Each row corresponds to a vessel. The first column gives the time at which the vessel arrives offshore (enters the queue), and the second column gives the time at which the vessel enters the harbour (leaves the queue).  A [NaT](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#datetimes) value in either of these columns indicates the vessel entered the queue prior to 2020, or left the queue after 2020, however this approach does not require these values to be NaT.  The third column gives the weight of cargo destined for the vessel.  Note, for the staircase approach to work we require every vessel, that was in the queue at some point in 2020, to appear in the dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(r\"data/vessel_queue.csv\", parse_dates=['enter', 'leave'], dayfirst=True)\r\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The layer method can be used with array-like parameters.  The creation of a step function to quantify the size of the queue is as simple as calling the layer method with a vector of times that vessels enter the queue, and a vector of times that vessels leave the queue - the columns \"enter\" and \"leave\" respectively:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue = sc.Stairs(data, \"enter\", \"leave\")\r\n",
    "queue.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assuming that no vessels arrive precisely at midnight on the 1st of Jan, we expect the number of vessels in the queue at this time to be equal to the number of NaT values in the \"enter\" column:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue(pd.Timestamp('2020-01-01'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.enter.isna().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Stairs class comes with a hist function (histogram) which can be pretty useful for understanding the distribution of the values of the corresponding step function.  The result is returned as a pandas.Series, indexed by a pandas.IntervalIndex.  If we don't supply the bins as a parameter it will use unit bins which cover the step function's range:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_hist = queue.clip(pd.Timestamp('2020'), pd.Timestamp('2021')).hist(stat=\"density\")\r\n",
    "queue_hist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know the queue length has to be a whole number so we can change the pandas.IntervalIndex accordingly before plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_hist.index = queue_hist.index.left\r\n",
    "ax = queue_hist.plot.bar()\r\n",
    "ax.set_xlabel('queue size', fontsize='12')\r\n",
    "ax.set_ylabel('frequency', fontsize='12');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another useful queue metric is the \"queue tonnes\".  This is the sum of the cargo tonnes destined for vessels in the queue.  A step function representing this variable is also straightforward by using the third parameter of the layer method - the values representing how much the step function should increase or decrease whenever the corresponding vessels enter or leave the queue:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_tonnes = sc.Stairs(data, \"enter\", \"leave\", \"tonnes\")\r\n",
    "queue_tonnes.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use this queue_tonnes object to answer questions like \"what was the maximum queue tonnes in 2020?\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_tonnes.max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ".. or \"what was the average size of the queue in 2020?\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_tonnes.clip(pd.Timestamp('2020'), pd.Timestamp('2021')).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "or \"what fraction of the year was the queue_tonnes larger than 1,500,000 tonnes?\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(queue_tonnes > 1500000).clip(pd.Timestamp('2020'), pd.Timestamp('2021')).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ".. or \"what was the median size of the queue in March 2020?\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_tonnes.clip(pd.Timestamp('2020-3-1'), pd.Timestamp('2020-4-1')).median()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The median gives us the 50th percentile, but we might be interested in the 80th percentile?  We can do that:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_tonnes.clip(pd.Timestamp('2020-3-1'), pd.Timestamp('2020-4-1')).percentile(80)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In fact we can even get a percentile function, represented by a Stairs object itself."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "percentile = queue_tonnes.clip(pd.Timestamp('2020-3-1'), pd.Timestamp('2020-4-1')).percentile\r\n",
    "percentile"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can plot this function of as it derives from a Stairs object:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "percentile.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 100th percentile should be the same as the maximum queue tonnes we found earlier.  Let's check:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_tonnes.percentile(100) == queue_tonnes.max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is the 40th, 65th, 77th and 90th percentiles?  The sample method, which is aliased by \\_\\_call\\_\\_, can be called with a vector of values at which to evaluate the step function too."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_tonnes.percentile([40, 65, 77, 90])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The percentile function is essentially the inverse of an [empirical cumulative distribution function](https://en.wikipedia.org/wiki/Empirical_distribution_function).  Perhaps, unsurprisingly we can generate an ecdf like so:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecdf = queue_tonnes.clip(pd.Timestamp('2020'), pd.Timestamp('2021')).ecdf\r\n",
    "ecdf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecdf.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Earlier we found that the queue tonnes were strictly above 150,000t about 9.87% of the time.  The ecdf can tell us what fraction of the time the queue tonnes was less than or equal to 150,000t.  (We'd expect these results to add to 1)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecdf(1500000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also then discover the fraction of time that 100,000 < queue tonnes <= 150,000 like so:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecdf(1500000) - ecdf(100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This sort of information is essentially a single bin in histogram data.  Earlier we used the Stairs.hist function on the queue length, with default bin sizes.  Since the range of the queue tonnes values is so large we will provide our own bins (defined by specifiying the edges) to generate a histogram for the queue tonnes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue_tonnes_hist = queue_tonnes.clip(pd.Timestamp('2020'), pd.Timestamp('2021')).hist(bins = range(0, 2100000, 100000), stat=\"density\")\r\n",
    "queue_tonnes_hist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we did earlier for the queue length histogram, we can quickly create a chart with pandas Series plotting functions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ax = queue_tonnes_hist.plot.bar()\r\n",
    "ax.set_xlabel('queue size in tonnes', fontsize='12')\r\n",
    "ax.set_ylabel('frequency', fontsize='12');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Returning to our queue plots... They're pretty noisy. We can reduce some of this noise by using the rolling_mean method belonging to the Stairs class.  The method uses the fact that a rolling mean of a piecewise constant function, i.e. a step function, will be a piecewise linear function.  And a piecewise linear function can be described by the (x,y) coordinates at each end of the \"pieces\".  The result is returned as a pandas.Series.\n",
    "\n",
    "In this example we apply a rolling mean with a 7 day, centred, window around every point in the step function.  We do this by specifying the window with a tuple, detailing the distances from the point to the boundaries of thw window - in this case, 3.5 days either side of the point."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue.rolling_mean(window=(-pd.Timedelta(3.5, 'D'), pd.Timedelta(3.5, 'D')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that our original data was contained to the year 2020 but the data in our rolling mean has spilled over into 2019, and 2020.  This is because our step function theoretically extends to -infinity and +infinity.  In this case this effect is unwanted.  Similar to the application of rolling means with pandas.Series, we will actually have to sacrifice some of our \"usable domain\" to create the rolling mean. We do this by specifying a domain of our step function when applying the rolling mean:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue.rolling_mean(\r\n",
    "    window=(-pd.Timedelta(3.5, 'D'), pd.Timedelta(3.5, 'D')),\r\n",
    "    where=(pd.Timestamp('2020'), pd.Timestamp('2021')),\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can see that we only have dates in our rolling mean data where the window entirely fits within the domain we specified.  The real use of this data is realised when plotting, and we can leverage the [plot function of pandas.Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.html) to this effect."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue.rolling_mean(\r\n",
    "    window=(-pd.Timedelta(3.5, 'D'), pd.Timedelta(3.5, 'D')),\r\n",
    "    where=(pd.Timestamp('2020'), pd.Timestamp('2021')),\r\n",
    ").plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that applying the rolling window has removed the noise and allowed us to look at trend.  We can get an ever coarser trend by using a larger window, for example 30 day centred window results in the following graph."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "queue.rolling_mean(\r\n",
    "    window=(-pd.Timedelta(15, 'D'), pd.Timedelta(15, 'D')),\r\n",
    "    where=(pd.Timestamp('2020'), pd.Timestamp('2021')),\r\n",
    ").plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another method to remove noise is to apply a static mean, rather than a rolling mean.  For example, we could use calculate a daily mean.  To achieve this let's use Python's zip function, list comprehension and a pandas.Series to derive and collect this data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "yr2020 = pd.date_range('2020', '2021')\r\n",
    "daily_mean_queue = pd.Series(\r\n",
    "    [queue.clip(d1,d2).mean() for d1,d2 in zip(yr2020[:-1], yr2020[1:])],\r\n",
    "    index = yr2020[:-1]\r\n",
    ")\r\n",
    "daily_mean_queue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A new way to achieve the exact same result, as of v1.5.0, is to use the sample method.  The sample method was used earlier to calculate the 40th, 65th, 77th and 90th percentiles for queue_tonnes, however we accessed it via its alias \\_\\_call\\_\\_ which allows us to simply write () instead of sample(). The sample method has been extended to be able to specify an aggregating function, and a window, similar to the rolling mean.  If we apply the mean function, and a 1 day leading window, then we get exactly the same result as the cell above (and it runs slightly faster too). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.Series(\r\n",
    "    queue.slice(yr2020).mean(),\r\n",
    "    index = yr2020[:-1],\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As with the rolling mean, this data is best interpreted with a plot:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "daily_mean_queue.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This time we can use [pandas.Series.rolling](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html) to get a rolling mean - although this will be a rolling mean of daily averages.  This data can of course be plotted with [matplotlib](https://matplotlib.org/) or [seaborn](https://seaborn.pydata.org/), but for now let's keep leveraging the pandas.Series plotting methods:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(20,5))\r\n",
    "daily_mean_queue.plot(ax=ax, label=\"queue size\")\r\n",
    "daily_mean_queue.rolling(7, center=True).mean().plot(ax=ax, linewidth=3, label=\"rolling mean\")\r\n",
    "ax.legend()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit ('staircase-wN5-eePl-py3.7': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "interpreter": {
   "hash": "6bf60174825840c504a5bcdeb584041e234e81e834f20bf858721befc2e0bedf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}